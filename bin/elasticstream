#!/usr/bin/env python

import os
import sys
import argparse
import json
import time

########################################################################################################################
# allow this script to be run from bin dir without install
########################################################################################################################
try:
    from elasticstream import ElasticStream
except ImportError:
    path = os.path.abspath(sys.argv[0])
    while os.path.dirname(path) != path:
        if os.path.exists(os.path.join(path, 'elasticstream', '__init__.py')):
            sys.path.insert(0, path)
            break
        path = os.path.dirname(path)
    from elasticstream import ElasticStream
########################################################################################################################

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('-u', '--url', help='Elasticsearch REST endpoint URL', default='localhost:9200')
parser.add_argument('-i', '--index', help='Elasticsearch index to stream (wildcards allowed)', default='*')
parser.add_argument('-k', '--keepalive', help='Duration to keep scroll alive. Tune according to --size', default='1m')
parser.add_argument('-s', '--size', help='Number of hits to scroll at once. Tune according to sharding', default='10')
parser.add_argument('-d', '--dsl', help='Elasticsearch DSL query or @file (e.g., "curl -d" syntax) containing query',
                    default='{"query": {"match_all": {}}}')
parser.add_argument('-f', '--format', choices=['jsonl', 'csv'], help='Stream format. JSON lines (jsonl) or CSV (csv)',
                    default='jsonl')
parser.add_argument('-o', '--output', help='Output file to stream to. If not stdout, prints progress to stdout',
                    default=sys.__stdout__)
args = parser.parse_args()

# allow curl -d @syntax
if args.dsl.startswith('@'):
    with open(args.dsl[1:]) as f:
        args.dsl = json.dumps(json.load(f))

# open the file once. we will flush periodically
if args.output is not sys.__stdout__:
    args.output = open(str(args.output), 'w')

stream = ElasticStream(args.url, args.index, args.dsl, scroll_keepalive=args.keepalive, scroll_size=args.size)
start_time = time.time()

if args.format == 'csv':
    args.output.write(','.join(stream.mappings)+'\n')

for hit in stream:
    # see http://jsonlines.org/ for more info
    if args.format == 'jsonl':
        args.output.write(hit+'\n')
        args.output.flush()

    # csv is slightly more complicated. need to keep track of headers and quote strings
    elif args.format == 'csv':
        # sort hit in header order
        hit_list = []
        for mapping in stream.mappings:
            if mapping in hit:
                if stream.mappings[mapping]['type'] == 'string':
                    # quote string in case it contains commas
                    hit_list.append('"%s"' % hit[mapping])
                else:
                    hit_list.append(hit[mapping])
            else:
                hit_list.append('')
        args.output.write(','.join(hit_list)+'\n')
        args.output.flush()

    # if aren't streaming to stdout, we can print metrics
    if args.output is not sys.__stdout__ and stream.hits_scrolled % int(args.size) == 0:
        elapsed_time = time.time() - start_time
        sys.stdout.write('%d/%d hits (%f/sec)\r' %
                         (stream.hits_scrolled, stream.hits_total, stream.hits_scrolled/elapsed_time))
        sys.stdout.flush()
        start_time = time.time()

args.output.close()
